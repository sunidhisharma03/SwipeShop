{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age  gender    occupation\n",
      "0        1   51  Female       Student\n",
      "1        2   67  Female       Student\n",
      "2        3   49  Female  Professional\n",
      "3        4   37  Female       Student\n",
      "4        5   59  Female  Professional\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Number of users\n",
    "num_users = 1000\n",
    "\n",
    "# Generate synthetic user data\n",
    "user_ids = np.arange(1, num_users + 1)\n",
    "user_ages = np.random.randint(18, 70, size=num_users)  # Ages between 18 and 70\n",
    "user_genders = np.random.choice(['Male', 'Female', 'Other'], size=num_users, p=[0.45, 0.45, 0.10])\n",
    "user_occupation = np.random.choice(['Student', 'Professional', 'Retired', 'Self-employed'], size=num_users, p=[0.3, 0.4, 0.2, 0.1])\n",
    "\n",
    "# Create DataFrame\n",
    "users = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'age': user_ages,\n",
    "    'gender': user_genders,\n",
    "    'occupation': user_occupation\n",
    "})\n",
    "\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78194453 0.075      0.36034412 0.         0.         0.\n",
      "  0.         0.36034412 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2319279\n",
      "  0.         0.         0.         0.         0.         0.18852711\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30076895 0.22272866 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30076895 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.20268963 0.44545732\n",
      "  0.         0.         0.30076895 0.         0.         0.\n",
      "  0.         0.         0.         0.30076895 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.92532264 0.925      0.         0.         0.         0.\n",
      "  0.24571604 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2227169\n",
      "  0.         0.         0.         0.23493555 0.         0.\n",
      "  0.         0.29213522 0.         0.         0.         0.24571604\n",
      "  0.         0.         0.49143209 0.         0.         0.24571604\n",
      "  0.         0.         0.         0.24571604 0.24571604 0.\n",
      "  0.         0.24571604 0.         0.         0.         0.\n",
      "  0.24571604 0.         0.         0.24571604 0.         0.\n",
      "  0.29213522 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.94847951 0.7        0.         0.         0.         0.\n",
      "  0.         0.         0.31416626 0.29697638 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31133848 0.         0.\n",
      "  0.         0.         0.24922696 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29820649 0.3085823  0.         0.35340519 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35340519 0.         0.\n",
      "  0.         0.         0.         0.         0.3085823  0.35340519\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.50592597 0.95       0.         0.34088107 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2032993\n",
      "  0.         0.         0.         0.26479405 0.         0.16525579\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34088107 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26364279 0.19523559 0.         0.\n",
      "  0.25701696 0.         0.         0.         0.         0.\n",
      "  0.         0.26364279 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19523559\n",
      "  0.         0.         0.26364279 0.         0.         0.\n",
      "  0.         0.         0.         0.52728557 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.1500233  0.275      0.         0.         0.         0.\n",
      "  0.         0.         0.3143017  0.29710441 0.         0.\n",
      "  0.31008528 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2493344  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29833505 0.30871532 0.         0.35355753 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355753 0.         0.\n",
      "  0.         0.         0.         0.         0.30871532 0.35355753\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assume that the following synthetic data and generate_description function are defined as previously discussed\n",
    "\n",
    "# Number of products\n",
    "num_products = 500\n",
    "\n",
    "# Generate synthetic product data\n",
    "product_ids = np.arange(1, num_products + 1)\n",
    "product_categories = np.random.choice(['Electronics', 'Books', 'Clothing', 'Home', 'Beauty', 'Sports'], size=num_products)\n",
    "product_prices = np.round(np.random.uniform(5.0, 500.0, size=num_products), 2)  # Prices between $5 and $500\n",
    "product_ratings = np.round(np.random.uniform(1.0, 5.0, size=num_products), 1)  # Ratings between 1.0 and 5.0\n",
    "\n",
    "# Keywords and templates for descriptions (assume already defined in previous examples)\n",
    "description_keywords = {\n",
    "    'Electronics': ['latest technology', 'high-performance', 'durable', 'compact', 'user-friendly'],\n",
    "    'Books': ['bestselling', 'engaging', 'classic', 'informative', 'must-read'],\n",
    "    'Clothing': ['stylish', 'comfortable', 'trendy', 'breathable', 'versatile'],\n",
    "    'Home': ['modern', 'elegant', 'cozy', 'durable', 'functional'],\n",
    "    'Beauty': ['premium quality', 'natural ingredients', 'long-lasting', 'hypoallergenic', 'luxurious'],\n",
    "    'Sports': ['high-performance', 'durable', 'lightweight', 'comfortable', 'professional-grade']\n",
    "}\n",
    "description_templates = {\n",
    "    'Electronics': [\n",
    "        \"This {category} item features {adjective1} design and {adjective2} functionality. It's {adjective3} and {adjective4}, perfect for everyday use.\",\n",
    "        \"Experience the {adjective1} of our {category} with this {adjective2} product. It's {adjective3} and {adjective4}, ensuring high performance.\"\n",
    "    ],\n",
    "    'Books': [\n",
    "        \"Dive into this {category} with our {adjective1} and {adjective2} read. It's {adjective3} and {adjective4}, a {adjective5} addition to any collection.\",\n",
    "        \"Our {category} offers a {adjective1} experience with {adjective2} insights. It's {adjective3} and {adjective4}, ideal for any reader.\"\n",
    "    ],\n",
    "    'Clothing': [\n",
    "        \"Our {category} is {adjective1} and {adjective2}, perfect for {adjective3} wear. It's {adjective4} and {adjective5}, making it a staple in any wardrobe.\",\n",
    "        \"This {category} combines {adjective1} style with {adjective2} comfort. It's {adjective3} and {adjective4}, suitable for all occasions.\"\n",
    "    ],\n",
    "    'Home': [\n",
    "        \"Enhance your living space with our {adjective1} and {adjective2} {category}. It's {adjective3} and {adjective4}, perfect for any home.\",\n",
    "        \"Our {category} is {adjective1} and {adjective2}, designed for {adjective3} use. It's {adjective4} and {adjective5}, adding a touch of elegance to your home.\"\n",
    "    ],\n",
    "    'Beauty': [\n",
    "        \"Experience {adjective1} and {adjective2} care with our {category}. It's {adjective3} and {adjective4}, perfect for a {adjective5} routine.\",\n",
    "        \"Our {category} features {adjective1} ingredients and {adjective2} results. It's {adjective3} and {adjective4}, ensuring a {adjective5} glow.\"\n",
    "    ],\n",
    "    'Sports': [\n",
    "        \"Achieve your best with our {adjective1} and {adjective2} {category}. It's {adjective3} and {adjective4}, perfect for {adjective5} performance.\",\n",
    "        \"This {category} is {adjective1} and {adjective2}, designed for {adjective3} activities. It's {adjective4} and {adjective5}, ideal for any sports enthusiast.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to generate a random description based on category\n",
    "def generate_description(category):\n",
    "    adjectives = random.sample(description_keywords[category], 5)\n",
    "    template = random.choice(description_templates[category])\n",
    "    return template.format(category=category, adjective1=adjectives[0], adjective2=adjectives[1], adjective3=adjectives[2], adjective4=adjectives[3], adjective5=adjectives[4])\n",
    "\n",
    "# Generate product descriptions\n",
    "product_descriptions = [generate_description(cat) for cat in product_categories]\n",
    "\n",
    "# Create DataFrame with descriptions\n",
    "products = pd.DataFrame({\n",
    "    'product_id': product_ids,\n",
    "    'category': product_categories,\n",
    "    'price': product_prices,\n",
    "    'rating': product_ratings,\n",
    "    'description': product_descriptions\n",
    "})\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "products[['price', 'rating']] = scaler.fit_transform(products[['price', 'rating']])\n",
    "\n",
    "# Vectorize product descriptions using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "product_descriptions_tfidf = vectorizer.fit_transform(products['description'])\n",
    "\n",
    "\n",
    "# Convert the sparse matrix to a dense array\n",
    "product_descriptions_dense = product_descriptions_tfidf.toarray()\n",
    "\n",
    "# Combine product metadata into a single feature set\n",
    "product_features = np.hstack((products[['price', 'rating']].values, product_descriptions_dense))\n",
    "\n",
    "# Example: Show combined feature set for first 5 products\n",
    "print(product_features[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  product_id interaction_type                     timestamp\n",
      "0      136         156             view 2023-01-01 00:00:00.000000000\n",
      "1      296         232             like 2023-01-01 01:16:11.017101710\n",
      "2      412         380            click 2023-01-01 02:32:22.034203420\n",
      "3      709         407            click 2023-01-01 03:48:33.051305130\n",
      "4      534         358             like 2023-01-01 05:04:44.068406840\n"
     ]
    }
   ],
   "source": [
    "# Number of interactions\n",
    "num_interactions = 10000\n",
    "\n",
    "# Generate synthetic interaction data\n",
    "interaction_user_ids = np.random.choice(user_ids, size=num_interactions)\n",
    "interaction_product_ids = np.random.choice(product_ids, size=num_interactions)\n",
    "interaction_types = np.random.choice(['view', 'click', 'purchase', 'like'], size=num_interactions, p=[0.6, 0.1, 0.01,0.29])\n",
    "interaction_timestamps = pd.date_range(start='2023-01-01', end='2024-06-13', periods=num_interactions)\n",
    "\n",
    "# Create DataFrame\n",
    "interactions = pd.DataFrame({\n",
    "    'user_id': interaction_user_ids,\n",
    "    'product_id': interaction_product_ids,\n",
    "    'interaction_type': interaction_types,\n",
    "    'timestamp': interaction_timestamps\n",
    "})\n",
    "\n",
    "print(interactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files\n",
    "users.to_csv('synthetic_users.csv', index=False)\n",
    "products.to_csv('synthetic_products.csv', index=False)\n",
    "interactions.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age  gender    occupation\n",
      "0        1   51  Female       Student\n",
      "1        2   67  Female       Student\n",
      "2        3   49  Female  Professional\n",
      "3        4   37  Female       Student\n",
      "4        5   59  Female  Professional\n",
      "   product_id category     price  rating  \\\n",
      "0           1   Sports  0.781945   0.075   \n",
      "1           2   Beauty  0.925323   0.925   \n",
      "2           3    Books  0.948480   0.700   \n",
      "3           4   Sports  0.505926   0.950   \n",
      "4           5    Books  0.150023   0.275   \n",
      "\n",
      "                                         description  \n",
      "0  Achieve your best with our high-performance an...  \n",
      "1  Our Beauty features premium quality ingredient...  \n",
      "2  Our Books offers a must-read experience with e...  \n",
      "3  This Sports is professional-grade and durable,...  \n",
      "4  Our Books offers a must-read experience with i...  \n",
      "   user_id  product_id interaction_type                      timestamp\n",
      "0      136         156             view  2023-01-01 00:00:00.000000000\n",
      "1      296         232             like  2023-01-01 01:16:11.017101710\n",
      "2      412         380            click  2023-01-01 02:32:22.034203420\n",
      "3      709         407            click  2023-01-01 03:48:33.051305130\n",
      "4      534         358             like  2023-01-01 05:04:44.068406840\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "users = pd.read_csv('synthetic_users.csv')  # Example file for user data\n",
    "products = pd.read_csv('synthetic_products.csv')  # Example file for product metadata\n",
    "interactions = pd.read_csv('synthetic_interactions.csv')  # Example file for user-product interactions\n",
    "\n",
    "# Explore datasets\n",
    "print(users.head())\n",
    "print(products.head())\n",
    "print(interactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "products[['price', 'rating']] = scaler.fit_transform(products[['price', 'rating']])\n",
    "\n",
    "# Vectorize product descriptions using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "# product_descriptions = vectorizer.fit_transform(products['description'])\n",
    "\n",
    "# Combine product metadata into a single feature set\n",
    "import numpy as np\n",
    "\n",
    "product_features = np.hstack((products[['price', 'rating']], product_descriptions_tfidf.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_19084\\3933847667.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interactions_dedup['interaction_type_binary'] = interactions_dedup['interaction_type'].apply(lambda x: 1 if x in ['view', 'click', 'purchase', 'like'] else 0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "interactions_dedup = interactions.drop_duplicates(subset=['user_id', 'product_id'])\n",
    "\n",
    "# Create user-item interaction matrix\n",
    "interactions_dedup['interaction_type_binary'] = interactions_dedup['interaction_type'].apply(lambda x: 1 if x in ['view', 'click', 'purchase', 'like'] else 0)\n",
    "user_item_matrix = interactions_dedup.pivot(index='user_id', columns='product_id', values='interaction_type_binary').fillna(0)\n",
    "\n",
    "# Create sparse matrix\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Apply Truncated SVD for matrix factorization\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "user_factors = svd.fit_transform(user_item_sparse)\n",
    "item_factors = svd.components_.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product_id category     price  rating  \\\n",
      "0             1   Sports  0.781945   0.075   \n",
      "398         399   Sports  0.824753   0.075   \n",
      "242         243   Sports  0.739784   0.075   \n",
      "301         302   Sports  0.748881   0.025   \n",
      "183         184   Sports  0.743593   0.125   \n",
      "388         389   Sports  0.839340   0.125   \n",
      "456         457   Sports  0.810916   0.175   \n",
      "28           29   Sports  0.659657   0.025   \n",
      "256         257   Sports  0.957576   0.200   \n",
      "461         462   Sports  0.584473   0.000   \n",
      "\n",
      "                                           description  \n",
      "0    Achieve your best with our high-performance an...  \n",
      "398  Achieve your best with our high-performance an...  \n",
      "242  Achieve your best with our high-performance an...  \n",
      "301  Achieve your best with our durable and profess...  \n",
      "183  Achieve your best with our comfortable and lig...  \n",
      "388  Achieve your best with our high-performance an...  \n",
      "456  Achieve your best with our professional-grade ...  \n",
      "28   Achieve your best with our durable and comfort...  \n",
      "256  Achieve your best with our durable and comfort...  \n",
      "461  Achieve your best with our professional-grade ...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between product features\n",
    "cosine_similarities = cosine_similarity(product_features)\n",
    "\n",
    "# Function to recommend similar products\n",
    "def recommend_products(product_id, top_n=10):\n",
    "    product_idx = products.index[products['product_id'] == product_id][0]\n",
    "    similar_indices = cosine_similarities[product_idx].argsort()[-top_n:][::-1]\n",
    "    similar_products = products.iloc[similar_indices]\n",
    "    return similar_products\n",
    "\n",
    "# Example usage\n",
    "recommended_products = recommend_products(product_id=1)\n",
    "print(recommended_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product_id  category     price  rating  \\\n",
      "230         231    Sports  0.974777   0.725   \n",
      "246         247    Beauty  0.988634   0.775   \n",
      "88           89    Beauty  0.946980   0.775   \n",
      "345         346  Clothing  0.961142   0.700   \n",
      "1             2    Beauty  0.925323   0.925   \n",
      "487         488    Beauty  0.820094   1.000   \n",
      "300         301    Beauty  0.933082   0.500   \n",
      "154         155    Beauty  0.981300   0.425   \n",
      "37           38    Beauty  0.855771   0.550   \n",
      "347         348    Beauty  0.952025   0.725   \n",
      "\n",
      "                                           description  \n",
      "230  Achieve your best with our comfortable and hig...  \n",
      "246  Experience natural ingredients and hypoallerge...  \n",
      "88   Experience hypoallergenic and luxurious care w...  \n",
      "345  Our Clothing is comfortable and stylish, perfe...  \n",
      "1    Our Beauty features premium quality ingredient...  \n",
      "487  Experience luxurious and hypoallergenic care w...  \n",
      "300  Experience luxurious and long-lasting care wit...  \n",
      "154  Experience luxurious and long-lasting care wit...  \n",
      "37   Experience luxurious and hypoallergenic care w...  \n",
      "347  Our Beauty features luxurious ingredients and ...  \n"
     ]
    }
   ],
   "source": [
    "# Function to get hybrid recommendations\n",
    "def hybrid_recommendations(user_id, top_n=10):\n",
    "    user_idx = users.index[users['user_id'] == user_id][0]\n",
    "    user_interactions = user_item_matrix.iloc[user_idx].values\n",
    "    \n",
    "    # Weighted average of collaborative filtering and content-based recommendations\n",
    "    cf_recommendations = user_factors[user_idx].dot(item_factors.T)\n",
    "    cb_recommendations = cosine_similarities.dot(user_interactions)\n",
    "    \n",
    "    combined_scores = cf_recommendations + cb_recommendations\n",
    "    top_indices = combined_scores.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    return products.iloc[top_indices]\n",
    "\n",
    "# Example usage\n",
    "user_recommendations = hybrid_recommendations(user_id=1)\n",
    "print(user_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  product_id interaction_type                      timestamp\n",
      "0      136         156             view  2023-01-01 00:00:00.000000000\n",
      "1      296         232             like  2023-01-01 01:16:11.017101710\n",
      "2      412         380            click  2023-01-01 02:32:22.034203420\n",
      "3      709         407            click  2023-01-01 03:48:33.051305130\n",
      "4      534         358             like  2023-01-01 05:04:44.068406840\n",
      "Unique values in 'interaction_type' before mapping:\n",
      "['view' 'like' 'click' 'purchase']\n",
      "   user_id  product_id  interaction_type                     timestamp  \\\n",
      "0      136         156                 1 2023-01-01 00:00:00.000000000   \n",
      "1      296         232                 2 2023-01-01 01:16:11.017101710   \n",
      "2      412         380                 3 2023-01-01 02:32:22.034203420   \n",
      "3      709         407                 3 2023-01-01 03:48:33.051305130   \n",
      "4      534         358                 2 2023-01-01 05:04:44.068406840   \n",
      "\n",
      "   decay_factor  interaction_value  \n",
      "0  1.061260e-23       1.061260e-23  \n",
      "1  1.172874e-23       2.345748e-23  \n",
      "2  1.172874e-23       3.518622e-23  \n",
      "3  1.172874e-23       3.518622e-23  \n",
      "4  1.172874e-23       2.345748e-23  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Sample interactions DataFrame for testing\n",
    "# data = {\n",
    "#     'user_id': [544, 719, 312, 367, 478],\n",
    "#     'product_id': [21, 487, 248, 161, 492],\n",
    "#     'interaction_type': ['view', 'like', 'view', 'click', 'purchase'],\n",
    "#     'timestamp': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-01-01', '2023-01-01']\n",
    "# }\n",
    "print(interactions.head())\n",
    "\n",
    "# interactions = pd.DataFrame(data)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "interactions['timestamp'] = pd.to_datetime(interactions['timestamp'])\n",
    "\n",
    "# Print unique values in 'interaction_type' before mapping\n",
    "print(\"Unique values in 'interaction_type' before mapping:\")\n",
    "print(interactions['interaction_type'].unique())\n",
    "\n",
    "# Define a mapping for interaction types to numeric values\n",
    "interaction_type_mapping = {\n",
    "    'view': 1,\n",
    "    'like': 2,\n",
    "    'click': 3,\n",
    "    'purchase': 4\n",
    "}\n",
    "\n",
    "# Map interaction_type to numeric values\n",
    "interactions['interaction_type'] = interactions['interaction_type'].map(interaction_type_mapping)\n",
    "\n",
    "# Check for unmapped values and handle them\n",
    "if interactions['interaction_type'].isnull().any():\n",
    "    print(\"Found unmapped values in 'interaction_type'. Handling them...\")\n",
    "    # Option 1: Drop rows with NaN interaction_type after mapping\n",
    "    interactions.dropna(subset=['interaction_type'], inplace=True)\n",
    "\n",
    "    # Option 2: Fill NaN with a default value (e.g., 0 for no interaction)\n",
    "    # interactions['interaction_type'].fillna(0, inplace=True)\n",
    "\n",
    "# Apply time decay factor\n",
    "max_time = interactions['timestamp'].max()\n",
    "\n",
    "def time_decay(t, max_time, decay_rate=0.1):\n",
    "    days_diff = (max_time - t).days\n",
    "    return np.exp(-decay_rate * days_diff)\n",
    "\n",
    "interactions['decay_factor'] = interactions['timestamp'].apply(lambda t: time_decay(t, max_time))\n",
    "\n",
    "# Multiply interaction_type with decay_factor to get interaction_value\n",
    "interactions['interaction_value'] = interactions['interaction_type'] * interactions['decay_factor']\n",
    "\n",
    "# Handle duplicate interactions if necessary\n",
    "interactions_dedup = interactions.drop_duplicates(subset=['user_id', 'product_id'])\n",
    "\n",
    "# Create user-item interaction matrix\n",
    "user_item_matrix = interactions_dedup.pivot(index='user_id', columns='product_id', values='interaction_value').fillna(0)\n",
    "\n",
    "# Convert DataFrame to sparse matrix\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Display the corrected DataFrame head\n",
    "print(interactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 9901]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# interactions_dedup_clean = interactions_dedup.dropna(subset=['interaction_type'])\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train a LinearRegression model on the user-item interaction matrix\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_item_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractions_dedup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minteraction_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, train_data, test_data):\n",
      "File \u001b[1;32mc:\\My Files\\KU\\5th sem\\Project\\SwipeShop\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\My Files\\KU\\5th sem\\Project\\SwipeShop\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\My Files\\KU\\5th sem\\Project\\SwipeShop\\.venv\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\My Files\\KU\\5th sem\\Project\\SwipeShop\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1291\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1273\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1274\u001b[0m     X,\n\u001b[0;32m   1275\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1287\u001b[0m )\n\u001b[0;32m   1289\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1291\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\My Files\\KU\\5th sem\\Project\\SwipeShop\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:460\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    458\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    463\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 9901]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(interactions, test_size=0.2, random_state=42)\n",
    "\n",
    "# interactions_dedup_clean = interactions_dedup.dropna(subset=['interaction_type'])\n",
    "\n",
    "# Train a LinearRegression model on the user-item interaction matrix\n",
    "model = LinearRegression()\n",
    "model.fit(user_item_sparse, interactions_dedup['interaction_type'])\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, train_data, test_data):\n",
    "    train_preds = model.predict(user_item_sparse)\n",
    "    test_preds = model.predict(test_data.pivot(index='user_id', columns='product_id').values)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_data['interaction'], train_preds))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_data['interaction'], test_preds))\n",
    "\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "# Example usage\n",
    "train_rmse, test_rmse = evaluate_model(model, train_data, test_data)\n",
    "print(f\"Train RMSE: {train_rmse}, Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/recommend', methods=['GET'])\n",
    "def recommend():\n",
    "    user_id = request.args.get('user_id')\n",
    "    top_n = int(request.args.get('top_n', 10))\n",
    "    \n",
    "    recommendations = hybrid_recommendations(user_id, top_n)\n",
    "    return jsonify(recommendations.to_dict(orient='records'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
